---
title: Build a Sample Application
---

<head>
    <meta name="title" content="Use Redpanda with Kafka Client Libraries | Redpanda Docs"/>
    <meta name="description" content="How to produce to and consume from Redpanda in a variety of languages."/>
    <link rel="canonical" href="https://docs.redpanda.com/docs/develop/code-examples/" />
</head>

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Redpanda is Kafka API-compatible, which means that despite Redpanda being a
new streaming data platform, you can leverage the countless client libraries
that have been created for Kafka. If you find something that is not
supported, reach out to our team in the community <a href="https://redpanda.com/slack">Slack</a>.

This example walks you through how to get started with a variety of Kafka
client libraries by creating a topic, producing some data, and consuming it
back.

## Set up Redpanda

Follow the [Redpanda Quickstart](../../get-started/quick-start) guide to spin up a development
environment in Docker. If you already have a Redpanda Cloud
cluster, then you're good to go; this example will show you how to connect to that too.

:::caution
If you're running Redpanda on your laptop, or in a shared development
environment, then avoid using Redpanda's optimized production settings. Running
`sudo rpk redpanda tune all` or manually configuring Redpanda for production
might affect your experience with other applications running on your machine.
:::

## Prepare the client environment

<Tabs groupId="lang">
<TabItem value="go" label="Go" default>

Download and install Go from [go.dev](https://go.dev/doc/install).
This Kafka client code example uses the [franz-go](https://github.com/twmb/franz-go) 
library.

```bash
# Create and enter the project folder
mkdir redpanda-go; cd redpanda-go
# Initialize the project
go mod init com/redpanda/example
# Install required dependencies
go get github.com/twmb/franz-go
go get github.com/twmb/franz-go/pkg/kadm
go get github.com/twmb/tlscfg
go get github.com/twmb/franz-go/pkg/sasl/scram@v1.9.0
```

</TabItem>
<TabItem value="js" label="Node.js" default>

Download and install Node.js [here](https://nodejs.org/en/download).
This example uses the [KafkaJS](https://kafka.js.org/) library.

```bash
# Create and enter the project folder
mkdir redpanda-node; cd redpanda-node
# Generate package.json (the default values are fine)
npm init
# Install required dependencies
npm i -D typescript
npm i -D @types/node
npm i kafkajs
# Generate tsconfig.json
tsc --init
```

</TabItem>
<TabItem value="py" label="Python" default>

Download and install Python 3 from 
[python.org](https://www.python.org/downloads). This example uses the
[kafka-python](https://kafka-python.readthedocs.io/en/master/) library.

```bash
# Create and enter the project folder
mkdir redpanda-python; cd redpanda-python
# Create virtual environment
python3 -m venv .env
source .env/bin/activate
# Install dependencies
(.env) pip install kafka-python
```

</TabItem>
</Tabs>

## Create topic

You can create a topic using Redpanda's command line interface 
[rpk](../../reference/rpk/rpk-commands):

<Tabs>
<TabItem value="local" label="Local" default>

```bash
rpk topic create demo
TOPIC  STATUS
demo   OK

rpk topic list
NAME  PARTITIONS  REPLICAS
demo  1           1
```

</TabItem>
<TabItem value="cloud" label="Cloud" default>

```bash
# Set the REDPANDA_BROKERS environment variable with the comma-separated list
# of cluster hosts. You can copy the list from the Overview tab in Redpanda 
# Cloud.
export REDPANDA_BROKERS="<TODO: change this to your cluster hosts>"

# Before running this command, create a service account in the Security tab in
# Redpanda Cloud and add the necessary ACLs to allow the service account to
# create, write to, and read from a topic with the name `demo`.
rpk topic create demo \
  --tls-enabled \
  --sasl-mechanism SCRAM-SHA-256 \
  --user "<TODO: change this to your service account name>" \
  --password "<TODO: change this to your service account password>"

TOPIC  STATUS
demo   OK
```

</TabItem>
</Tabs>

You can also create a topic programmatically:

<Tabs groupId="lang">
<TabItem value="go" label="Go" default>

```go title="admin.go"
package main

import (
  "context"
  "fmt"
  "github.com/twmb/franz-go/pkg/kadm"
  "github.com/twmb/franz-go/pkg/kgo"
)

func main() {
  topic := "demo"
  seeds := []string{"localhost:9092"}

  client, err := kgo.NewClient(
    kgo.SeedBrokers(seeds...),
  )
  if err != nil {
    panic(err)
  }
  defer client.Close()

  admin := kadm.NewClient(client)
  defer admin.Close()

  ctx := context.Background()
  // Create a topic with a single partition and single replica
  resp, _ := admin.CreateTopics(ctx, 1, 1, nil, topic)
  for _, ctr := range resp {
    if ctr.Err != nil {
      fmt.Printf("Unable to create topic '%s': %s", ctr.Topic, ctr.Err)
    } else {
      fmt.Printf("Created topic '%s'", ctr.Topic)
    }
  }
}
```

</TabItem>
<TabItem value="js" label="Node.js" default>

```javascript title="admin.ts"
const {Kafka} = require("kafkajs")

const redpanda = new Kafka({brokers: ["localhost:9092"]})
const admin = redpanda.admin()

admin.connect().then(() => {
  admin.createTopics({
    topics: [{
      topic: "demo",
      numPartitions: 1,
      replicationFactor: 1
    }]
  })
  .then((resp) => {
    resp ? console.log("Created topic") :
      console.log("Failed to create topic")
  })
  .finally(() => admin.disconnect())
})
```

</TabItem>
<TabItem value="py" label="Python" default>

```python title="admin.py"
from kafka import KafkaAdminClient
from kafka.admin import NewTopic
from kafka.errors import TopicAlreadyExistsError

admin = KafkaAdminClient(bootstrap_servers="localhost:9092")

try:
  demo_topic = NewTopic(name="demo", num_partitions=1, replication_factor=1)
  admin.create_topics(new_topics=[demo_topic])
  print("Created topic")
except TopicAlreadyExistsError as e:
  print("Topic already exists")
finally:
  admin.close()
```

</TabItem>
</Tabs>

## Connecting to Redpanda Cloud

Connecting to a local Redpanda cluster (or a cluster with no security) is as 
simple as specifying a list of brokers; however, this is done differently in Redpanda
Cloud.

You can configure Redpanda Cloud to use SASL/SCRAM (username and password) or
mTLS based [authentication](../../manage/security/authentication). These modes of
security require some additional parameters to be specified when creating a
client connection.

:::info
Redpanda Cloud environments use certificates signed by
[Let's Encrypt](https://letsencrypt.org/). Most programming languages will
load their root certificate authority (`ISRG Root X1`) by default so you
shouldn't need to provide a custom CA.
:::

<Tabs groupId="lang">
<TabItem value="go" label="Go" default>

```go
package main

import (	
  "crypto/tls"
  "github.com/twmb/franz-go/pkg/kgo"	
  "github.com/twmb/franz-go/pkg/sasl/scram"
)

func main() {
  seeds := []string{"<TODO: change this to your cluster hosts>"}

  opts := []kgo.Opt{}
  opts = append(opts,
    kgo.SeedBrokers(seeds...),
  )

  // Initialize public CAs for TLS
  opts = append(opts, kgo.DialTLSConfig(new(tls.Config)))

  /* Initialize mTLS
  tc, err := tlscfg.New(
    // Custom CA is only required if you are using self-signed certificates
    tlscfg.MaybeWithDiskCA("ca.crt", tlscfg.ForClient),
    tlscfg.MaybeWithDiskKeyPair("client.crt", "client.key"),
  )
  if err != nil {
    panic(err)
  }
  opts = append(opts, kgo.DialTLSConfig(tc))
  */

  // Initializes SASL/SCRAM
  opts = append(opts, kgo.SASL(scram.Auth{
    User: "<TODO: change this to your service account name>",
    Pass: "<TODO: change this to your service account password>",
  }.AsSha256Mechanism()))

  client, _ := kgo.NewClient(opts...)
  defer client.Close()
}
```

</TabItem>
<TabItem value="js" label="Node.js" default>

```javascript
const {Kafka} = require("kafkajs")

const redpanda = new Kafka({
  brokers: ["<TODO: change this to your cluster hosts>"],
  ssl: {
    // mTLS client certificate and private key can be downloaded from the
    // Overview tab in the Redpanda Cloud UI:
    // cert: fs.readFileSync("client.crt", "utf8"),
    // key: fs.readFileSync("client.key", "utf8"),

    // Custom CA is only required if you are using self-signed certificates
    // ca: fs.readFileSync("ca.crt", "utf8")
    },
    sasl: {
      mechanism: "scram-sha-256",
      username: "<TODO: change this to your service account name>",
      password: "<TODO: change this to your service account password>"
    }
})
```

</TabItem>
<TabItem value="py" label="Python" default>

```python
from kafka import KafkaProducer

producer = KafkaProducer(
  bootstrap_servers="<TODO: change this to your cluster hosts>",
  security_protocol="SASL_SSL",
  sasl_mechanism="SCRAM-SHA-256",
  sasl_plain_username="<TODO: change this to your service account name>",
  sasl_plain_password="<TODO: change this to your service account password>",
  
  # mTLS client certificate and private key can be downloaded from the
  # Overview tab in the Redpanda Cloud UI:
  # ssl_certfile="client.crt",
  # ssl_keyfile="client.key",

  # Custom CA is only required if you are using self-signed certificates
  # ssl_cafile="ca.crt"
)
```

</TabItem>
</Tabs>

## Create producer

After you have a topic, you can create a producer and send some messages:

<Tabs groupId="lang">
<TabItem value="go" label="Go" default>

```go title="producer.go"
package main

import (
  "context"
  "fmt"
  "os"
  "sync"
  "github.com/twmb/franz-go/pkg/kgo"
)

func main() {
  topic := "demo"
  hostname, _ := os.Hostname()
  ctx := context.Background()
  seeds := []string{"localhost:9092"}

  client, err := kgo.NewClient(
    kgo.SeedBrokers(seeds...),
  )
  if err != nil {
    panic(err)
  }
  defer client.Close()

  var wg sync.WaitGroup
  for i := 1; i < 100; i++ {
    wg.Add(1)
    record := &kgo.Record {
      Topic: topic,
      Key: []byte(hostname),
      Value: []byte(fmt.Sprintf("Message %d", i)),
    }
    client.Produce(ctx, record, func(record *kgo.Record, err error) {
      defer wg.Done()
      if err != nil {
        fmt.Printf("Error sending message: %v \n", err)
      } else {
        fmt.Printf("Message sent: topic: %s, offset: %d, value: %s \n", 
          topic, record.Offset, record.Value)
      }
    })
  }
  wg.Wait()

  // Alternatively, produce messages synchronously 
  for i := 100; i < 200; i++ {
    record := &kgo.Record{
      Topic: topic,
      Key: []byte(hostname),
      Value: []byte(fmt.Sprintf("Synchronous message %d", i)),
    }
    results := client.ProduceSync(ctx, record)
    for _, pr := range results {
      if pr.Err != nil {
        fmt.Printf("Error sending synchronous message: %v \n", pr.Err)
      } else {
        fmt.Printf("Message sent: topic: %s, offset: %d, value: %s \n", 
          topic, pr.Record.Offset, pr.Record.Value)
      }
    }
  }
}
```

</TabItem>
<TabItem value="js" label="Node.js" default>

```javascript title="producer.ts"
const os = require("os")
const {Kafka, CompressionTypes} = require("kafkajs")

const redpanda = new Kafka({brokers: ["localhost:9092"]})
const producer = redpanda.producer()

const sendMessage = (msg: string) => {
  return producer.send({
    topic: "demo",
    compression: CompressionTypes.GZIP,
    messages: [{
      // Messages with the same key are sent to the same topic partition for
      // guaranteed ordering
      key: os.hostname(),
      value: JSON.stringify(msg)
    }]
  })
  .catch((e) => {
    console.error(`Unable to send message: ${e.message}`, e)
  })
}

const run = async () => {
  await producer.connect()
  for (let i = 0; i < 100; i++) {
    sendMessage(`message ${i}`).then((resp) => {
      console.log(`Message sent: ${JSON.stringify(resp)}`)
    })
  }
}

run().catch(console.error)

process.once("SIGINT", async () => {
  try {
    await producer.disconnect()
    console.log("Producer disconnected")
  } finally {
    process.kill(process.pid, "SIGINT")
  }
})
```

</TabItem>
<TabItem value="py" label="Python" default>

```python title="producer.py"
import socket
from kafka import KafkaProducer
from kafka.errors import KafkaError

producer = KafkaProducer(bootstrap_servers="localhost:9092")
hostname = str.encode(socket.gethostname())

# Produce asynchronously
for i in range(100):
  msg = f"message #{i}"
  producer.send(
    "demo",
    key=hostname,
    value=str.encode(msg)
  )
producer.flush()

def on_success(metadata):
  print(f"Sent to topic '{metadata.topic}' at offset {metadata.offset}")

def on_error(e):
  print(f"Error sending message: {e}")

# Produce asynchronously with callbacks
for i in range(100, 200):
  msg = f"message with callbacks #{i}"
  future = producer.send(
    "demo",
    key=hostname,
    value=str.encode(msg)
  )
  future.add_callback(on_success)
  future.add_errback(on_error)
producer.flush()

# Wait for every future to produce synchronously
for i in range(200, 300):
  msg = f"synchronous message #{i}"
  future = producer.send(
    "demo",
    key=hostname,
    value=str.encode(msg)
  )
  try:
    metadata = future.get(timeout=5)
    print(f"Sent to topic '{metadata.topic}' at offset {metadata.offset}")
  except KafkaError as e:        
    print(f"Error sending message: {e}")
    pass
```

</TabItem>
</Tabs>

## Create consumer

You can create a consumer to read the data back out of the topic:

<Tabs groupId="lang">
<TabItem value="go" label="Go" default>

```go title="consumer.go"
package main

import (
  "context"
  "fmt"
  "github.com/twmb/franz-go/pkg/kgo"
)

func main() {
  topic := "demo"
  ctx := context.Background()
  seeds := []string{"localhost:9092"}

  client, err := kgo.NewClient(
    kgo.SeedBrokers(seeds...),
    kgo.ConsumerGroup("demo-group"),
    kgo.ConsumeTopics(topic),
    kgo.ConsumeResetOffset(kgo.NewOffset().AtStart()),
  )
  if err != nil {
    panic(err)
  }
  defer client.Close()

  for {
    fetches := client.PollFetches(ctx)
    if errs := fetches.Errors(); len(errs) > 0 {
      // All errors are retried internally when fetching, but non-retriable
      // errors are returned from polls so that users can notice and take
      // action.
      panic(fmt.Sprint(errs))
    }

    iter := fetches.RecordIter()
    for !iter.Done() {
      record := iter.Next()
      topicInfo := fmt.Sprintf("topic: %s (%d|%d)",
        record.Topic, record.Partition, record.Offset)
      messageInfo := fmt.Sprintf("key: %s, Value: %s",
        record.Key, record.Value)      
      fmt.Printf("Message consumed: %s, %s \n", topicInfo, messageInfo)
    }
  }
}
```

</TabItem>
<TabItem value="js" label="Node.js" default>

```javascript title="consumer.ts"
const {Kafka} = require("kafkajs")

const redpanda = new Kafka({brokers: ["localhost:9092"]})
const consumer = redpanda.consumer({groupId: "demo-group"})

const run = async () => {
  await consumer.connect()
  await consumer.subscribe({
    topic: "demo",
    fromBeginning: true
  })
  await consumer.run({
    eachMessage: async ({topic, partition, message}) => {
      const topicInfo = `topic: ${topic} (${partition}|${message.offset})`
      const messageInfo = `key: ${message.key}, value: ${message.value}`
      console.log(`Message consumed: ${topicInfo}, ${messageInfo}`)
    },
  })
}

run().catch(console.error)

process.once("SIGINT", async () => {
  try {
    await consumer.disconnect()
    console.log("Consumer disconnected")
  } finally {
    process.kill(process.pid, "SIGINT")
  }
})
```

</TabItem>
<TabItem value="py" label="Python" default>

```python title="consumer.py"
from kafka import KafkaConsumer

consumer = KafkaConsumer(
  bootstrap_servers=["localhost:9092"],
  group_id="demo-group",
  auto_offset_reset="earliest",
  enable_auto_commit=False,
  consumer_timeout_ms=1000
)
consumer.subscribe("demo")

for message in consumer:
  topic_info = f"topic: {message.topic} ({message.partition}|{message.offset})"
  message_info = f"key: {message.key}, {message.value}"
  print(f"{topic_info}, {message_info}")
```

</TabItem>
</Tabs>

## Running

<Tabs groupId="lang">
<TabItem value="go" label="Go" default>

```bash
# Create the topic
go run admin.go
# Produce some data
go run producer.go
# Consume it back
go run consumer.go
```

</TabItem>
<TabItem value="js" label="Node.js" default>

```bash
# Create the topic
tsc admin.ts && node admin.js
# Produce some data
tsc producer.ts && node producer.js
# Consume it back
tsc consumer.ts && node consumer.js
```

</TabItem>
<TabItem value="py" label="Python" default>

```bash
# Create the topic
(.env) python3 admin.py
# Produce some data
(.env) python3 producer.py
# Consume it back
(.env) python3 consumer.py
```

</TabItem>
</Tabs>

## Wrapping up

In this example you developed the building blocks of a Redpanda client
application that creates a topic, produces messages to, and consumes messages
from a Redpanda cluster running in a local environment, or in Redpanda Cloud.

The code provided here is intentionally simple to help you get
started. For additional resources to help you build stream processing
applications that can aggregate, join, and filter your data streams, see:

* [Redpanda University](https://university.redpanda.com/)
* [Redpanda Blog](https://redpanda.com/blog)
* [Resources](https://redpanda.com/resources)
